---
title: "ST 501 Final Project Q3 Write Up"
author: "Eric Warren"
date: "`r Sys.Date()`"
urlcolor: blue
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.pos = "H", out.extra = "")
options(scipen = 999)
```

```{r output setup, eval=FALSE, echo=FALSE}
# This code allows us to render a pdf document
rmarkdown::render("~/ST-501-Final-Project/ST 501 Final Project Q3 Write Up.Rmd", 
              output_format = "pdf_document", 
              output_options = list(
                toc = TRUE, 
                toc_depth = 3,
                number_sections = FALSE,
                extra_dependencies = "float"
                )
              )
```

# Introduction

Here we are going to let $X_1$ and $X_2$ be random variables with $E[X_i] = \mu_i$ and Var$(X_i) = \sigma_i^2$ for i = 1, 2. We are going to assume that $0 < \sigma_1^2 \leq \sigma_2^2$. We are also going to define our weighted random variable X(w), which will end up being our portfolio between two stocks. We will say that $X(w) = wX_1 + (1-w) X_2$ for $w \in [0, 1]$ and $\rho$ is the correlation between $X_1$ and $X_2$.

# Part A

For any $w \in [0,1]$, find $c_0, c_1, a, b$, and $c$ such that $E(X(w)) = \mu(w) = c_0 + c_1 w$ and Var$(X(w)) = \sigma^2(w) = aw^2 - 2bw + c$.

Here we can first find the $E(X(w)) = E(wX_1 + (1-w) X_2) = E(wX_1) + E((1-w) X_2) = w E(X_1) + (1-w) E(X_2) = w\mu_1 + (1-w) \mu_2 = w\mu_1 + \mu_2 - w\mu_2 = \mu_2 + w (\mu_1 - \mu_2)$. Therefore, $c_0 = \mu_2$ and $c_1 = \mu_1 - \mu_2$. 

Now we can find Var$(X(w)) = Var(w X_1 + (1-w) X_2) = w^2 Var(X_1) + (1-w)^2 Var(X_2) + 2Cov(xX_1, (1-w) X_2) = w^2 \sigma_1^2 + (w^2 - 2w + 1) \sigma_2^2 + 2w(1-w) Cov(X_1, X_2) = w^2 \sigma_1^2 + (w^2 - 2w + 1) \sigma_2^2 + (2w - 2w^2) (\rho \sigma_1 \sigma_2) = w^2 \sigma_1^2 + w^2 \sigma_2^2 - 2w \sigma_2^2 + \sigma_2^2 + 2w \rho \sigma_1 \sigma_2 - 2w^2 \rho \sigma_1 \sigma_2 = w^2 (\sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2) - 2w (\sigma_2^2 - \rho \sigma_1 \sigma_2) + \sigma_2^2$. Also if we say that $\rho \sigma_1 \sigma_2 = Cov(X_1, X_2)$ then our final answer of $w^2 (\sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2) - 2w (\sigma_2^2 - \rho \sigma_1 \sigma_2) + \sigma_2^2 = w^2 (\sigma_1^2 + \sigma_2^2 - 2 Cov(X_1, X_2)) - 2w (\sigma_2^2 - Cov(X_1, X_2)) + \sigma_2^2$. Therefore, we can clearly see that $a = \sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2 = \sigma_1^2 + \sigma_2^2 - 2 Cov(X_1, X_2), b = \sigma_2^2 - \rho \sigma_1 \sigma_2 = \sigma_2^2 - Cov(X_1, X_2)$, and $c = \sigma_2^2$.

# Part B

Suppose we want to find $w \in [0, 1]$ that maximizes $\mu(w)$ subject to the
constraint that $\sigma^2(w) \leq \sigma_1^2$. 

## Part i

If $\mu_1 \geq \mu_2$, show that w = 1 solves the optimization.

Note that $E(X(w)) = \mu_2 + w (\mu_1 - \mu_2)$ so if we want the highest expected return (aka larger $E(X(w))$) then we want w to be as high as possible fitting our constraint to minimize the effect $\mu_2$ has on the equation and maximize the effect $\mu_1$ has since we know that $\mu_1 \geq \mu_2$. Note the highest w we can have is 1 because $w \in [0, 1]$. If w = 1 (the maximum value), then our variance $\sigma^2(w) = w^2 (\sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2) - 2w (\sigma_2^2 - \rho \sigma_1 \sigma_2) + \sigma_2^2 = 1^2 (\sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2) - 2*1 (\sigma_2^2 - \rho \sigma_1 \sigma_2) + \sigma_2^2 = \sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2 - 2 \sigma_2^2 + 2 \rho \sigma_1 \sigma_2 + \sigma_2^2 = \sigma_1^2 + (1-2+1) \sigma_2^2 + (-2+2) \rho \sigma_1 \sigma_2 = \sigma_1^2 + 0 \sigma_2^2 + 0 \rho \sigma_1 \sigma_2 = \sigma_1^2$. Now to check the constraint, $\sigma^2(w) = \sigma_1^2 \leq \sigma_1^2$ is a true statement, so we have shown that w = 1 maximizes our $\mu(w)$.

## Part ii

If $\mu_1 < \mu_2$, show that the optimal w is given by the smallest $w \in [0,1]$ that satisfies the constraint $\sigma^2(w) \leq \sigma_1^2$.

Note that $E(X(w)) = \mu_2 + w (\mu_1 - \mu_2)$. This is very similar to **Part i** but the reverse. Now if we want the highest expected return (aka larger $E(X(w))$) then we want w to be as low as possible fitting our constraint to minimize the effect $\mu_1$ has on the equation and maximize the effect $\mu_2$ has since we know that $\mu_2 > \mu_1$. Therefore, the optimal w has to be the smallest $w \in [0,1]$ that satisfies the constraint $\sigma^2(w) \leq \sigma_1^2$ in order for this to occur.

# Part C

We can use $X_1$ and $X_2$ to model two stocks in a portfolio. Please note that this part was done all in R. We will detail our findings in this report and skip explaining many of the steps (especially the parts with code). To view the code that went on behind the scenes to please view this attachment by clicking [the link to the R code in the introduction section](https://github.com/ericwarren9/ST-501-Final-Project/blob/main/Problem%203%20C.R). 

## Part i

Estimate $c_0, c_1, a, b$, and $c$ using the moment estimates of the AAPL and
MSFT return prices.

The first step is understanding which stock represents $X_1$ with the other representing $X_2$. Remember we are told that $0 < \sigma_1^2 \leq sigma_2^2$. So we should try to find the stock with the lower variance and that will represent our $X_1$ value with the higher variance stock representing $X_2$. After doing this, we can say that MSFT represents $X_1$ and AAPL represents $X_2$. So now knowing this we can find $\mu_1$ which is MSFT's mean, $\mu_2$ which is AAPL's mean, $\sigma_1^2$ which is MSFT's variance and $\sigma_2^2$ which is AAPL's variance. We found these values to be $\mu_1 = 0.001118, \mu_2 = 0.001130, \sigma_1^2 = 0.0003831200$, and $\sigma_2^2 = 0.0004356924$. We also found the Cov$(X_1, X_2) = 0.0003105146$. 

I found this interesting but using the values of we can find our correlation between both stocks to be $\rho = \frac{Cov(X_1, X_2)}{\sqrt{\sigma_1^2 * \sigma_2^2}} = \frac{0.0003105146}{\sqrt{0.0003831200 * 0.0004356924}} = 0.7600193$. So we can see that the returns of AAPL and MSFT are highly correlated. 

To solve for the values, we know that $c_0 = \mu_2 = 0.001130$ and $c_1 = \mu_1 - \mu_2 = 0.001118 - 0.001130 = -0.000012$. We also know that $a = \sigma_1^2 + \sigma_2^2 - 2 \rho \sigma_1 \sigma_2 = \sigma_1^2 + \sigma_2^2 - 2 Cov(X_1, X_2) = 0.0003831200 + 0.0004356924 - 2(0.0003105146) = 0.0001977832$, and then $b = \sigma_2^2 - \rho \sigma_1 \sigma_2 = \sigma_2^2 - Cov(X_1, X_2) = 0.0004356924 - 0.0003105146 = 0.0001251778$, and lastly $c = \sigma_2^2 = 0.0004356924$.


## Part ii

Estimate the optimal w according to **Part B**.

There are two cases we need to find, when $\mu_1 \geq \mu_2$ and when $\mu_2 > \mu_1$. For the first case $\mu_1 \geq \mu_2$, we already proved that the optimal w = 1. Please feel free to reference **Part B part i** to see how this is the case.

Now for our second case $\mu_2 > \mu_1$, we want w to be as low as possible while still meeting the condition that $\sigma^2(w) \leq \sigma_1^2$. This was something we proved in **Part B part ii**. So what we can do set the inequality $\sigma^2(w) \leq \sigma_1^2$ and solve for what the range of w is. Then we can find the lowest value for w and that is our optimal w in this case. So we know what our inequality is:

$$\sigma^2(w) \leq \sigma_1^2$$
$$w^2 (\sigma_1^2 + \sigma_2^2 - 2 Cov(X_1, X_2)) - 2w (\sigma_2^2 - Cov(X_1, X_2)) + \sigma_2^2 \leq \sigma_1^2$$
$$w^2 (0.0003831200 + 0.0004356924 - 2(0.0003105146)) - 2w (0.0004356924 - 0.0003105146) + 0.0004356924 \leq 0.0003831200$$
$$0.0001977832 w^2 - 0.0002503556 w + 0.0004356924 \leq 0.0003831200$$
$$0.0001977832 w^2 - 0.0002503556 w + 0.0004356924 - 0.0003831200 \leq 0$$
$$0.0001977832 w^2 - 0.0002503556 w + 0.0000525724 \leq 0$$

Now we can say that the equality of $0.0001977832 w^2 - 0.0002503556 w + 0.0000525724 = 0$ fits the constraints of $0.0001977832 w^2 - 0.0002503556 w + 0.0000525724 \leq 0$. This equality which is in the form of $dw^2 + ew + f = 0$ also allows us to solve for w by using the quadratic formula which if you do not remember is $w = \frac{-e \pm \sqrt{e^2 - 4 d f}}{2 d}$. In our case, $d = 0.0001977832, e = - 0.0002503556$, and $f = 0.0000525724$. So we can plug in these numbers into our quadratic formula to get $w = \frac{-(- 0.0002503556) \pm \sqrt{(- 0.0002503556)^2 - 4(0.0001977832)(0.0000525724)}}{2(0.0001977832)} = \frac{0.0002503556 \pm \sqrt{0.00000006267793 - 0.00000004159175}}{0.0003955664} = \frac{0.0002503556 \pm \sqrt{0.00000002108618}}{0.0003955664}$. Therefore, our solutions are $w = \frac{0.0002503556 - \sqrt{0.00000002108618}}{0.0003955664} = 0.2658$ and $w = \frac{0.0002503556 + \sqrt{0.00000002108618}}{0.0003955664} = 1$. This means that in order for the constraint of $\sigma^2(w) \leq \sigma_1^2$ to hold, $w \in [0.2658, 1]$. Since we have already said that the optimal w is the lowest value that meets the constraint, then our optimal w has to be equal to 0.2658.

## Part iii

Estimate the value-at-risk (VaR) separately for both AAPL and MSFT with
confidence $\alpha = 0.95$. Again please reference 

For calculating the VaR for each stock separately, we are first going to explain what VaR is. This is the risk of our loses and we calculate it by finding the lowest $1 - \alpha$ quantile of data and saying that we have $100 \alpha$% confidence that value at risk (or what we could expect to lose) is the $1 - \alpha$ quantile value. Now we are told that $\alpha = 0.95$ so we want to find the $1 - \alpha = 1 - 0.95 = 0.05$ quantile of a specific stock's returns. That 0.05 (or 5%) quantile value will be our VaR.

So first we start with MSFT. We found the 5% quantile of MSFT returns is -0.02961778. This means with 95% confidence our losses will not exceed the amount of about 2.96% of our MSFT stock portfolio's worth in a given day or our VaR is about 2.96%. For example if our portfolio for MSFT was one million dollars then our VaR using this calculation would be about 29,600 dollars or with 95% confidence our losses will not exceed the amount of about 29,600 dollars of MSFT stock in a given day.

Now let us evaluate AAPL. We found the 5% quantile of AAPL returns is -0.03224605. This means with 95% confidence our losses will not exceed the amount of about 3.22% of our AAPL stock portfolio's worth in a given day or our VaR is about 3.22%. For example if our portfolio for AAPL was one million dollars then our VaR using this calculation would be about 33,200 dollars or with 95% confidence our losses will not exceed the amount of about 33,200 dollar of AAPL stock in a given day.

## Part iv

Find w that minimizes the VaR for the combined portfolio of AAPL and
MSFT for $\alpha = 0.95$.

Remember that our combined portfolio returns are $X_2 + w (X_1 - X_2)$ where $X_1$ is MSFT and $X_2$ is AAPL Therefore, Now also know that $w \in [0.2658, 1]$. So what we did was do a simulation of the 5% quantile of this portfolio's returns and then find which had the best VaR (in this case we are trying to find which 5% quantile negative return value is as close to 0 -- or not losing anything -- as possible). We did the simulation increasing the value of w by 0.0001 in the interval of [0.2658, 1] and found that when w is 0.6891 it had the best VaR value of -0.02803917. That means if our goal is to minimize our VaR we should choose our w value to be 0.6891, or in investing terms we should get 0.6891 units of MSFT and 0.3109 units of AAPL per 1 unit in our portfolio of these two stocks. When doing this, we have 95% confidence our losses will not exceed the amount of about 2.80% of our portfolio's worth in a given day.

# Question 2Ai

Let $X \sim$ Exp$(\lambda = 1)$. Show that the minimum order statistic converges to 0 as $n \to \infty$.

First let us remember the density for an order statistic. It is $f_{(k)}(X) = \frac{n!}{(n-1)!} (f(x)) (F(x))^{k-1} (1 - F(x))^{n-k} = n (f(x)) (F(x))^{k-1} (1 - F(x))^{n-k}$. Now in our case we are trying to find the minimum order statistic which is $X_{(1)}$ so we can see that $k = 1$. Also note that for $X \sim$ Exp$(\lambda = 1)$ $f(x) = \lambda e^{-\lambda x}$ and $F(x) = 1 - e^{-\lambda x}$. Also note that $\lambda = 1$ in our function's case which we can plug in later. Therefore we can plug in our formula to get 

$$f_{X_{(1)}}(X_{(1)})$$
$$= n (f(x)) (F(x))^{k-1} (1 - F(x))^{n-k}$$ 
$$= n (f(x)) (F(x))^{1-1} (1 - F(x))^{n-1}$$
$$= n (f(x)) (1 - F(x))^{n-1}$$ 
$$= n (\lambda e^{\lambda x}) (1 - (1 - e^{\lambda x}))^{n - 1}$$
$$= n (1 e^{-1x}) (1 + -1 + e^{-1 x})^{n - 1}$$
$$= n (e^{-x}) (e^{-x})^{n - 1}$$
$$= n (e^{-x})^n$$
$$= ne^{-nx}$$

Now note that we get $f_{X_{(1)}}(X_{(1)}) = ne^{-nx}$ which if we treat n as $\lambda$ then we just have another exponential random variable from this where $X_{(1)} \sim$ Exp(n). Anyways, to the minimum order statistic converges to 0 as $n \to \infty$, we have to show that its expected value and variance both converge to 0 as $n \to \infty$. 

First, we will show the expected value converges. Since we said that the minimum order statistic is just another exponential random variable, we know its expected value is just $E(X_{(1)}) = \frac{1}{n}$ (since for exponential random variables when $X \sim$ Exp$(\lambda)$, $E(X) = \frac{1}{\lambda}$). Now we can take the limit as $\lim_{n\to\infty} E(X_{(1)}) = \lim_{n\to\infty} \frac{1}{n} = \frac{1}{\infty} = 0$. So we have shown that its expected value converges to 0.

Now we will show its variance converges. Since we said that the minimum order statistic is just another exponential random variable, we know its variance is just $V(X_{(1)}) = \frac{1}{n^2}$ (since for exponential random variables when $X \sim$ Exp$(\lambda)$, $Var(X) = \frac{1}{\lambda^2}$). Now we can take the limit as $\lim_{n\to\infty} Var(X_{(1)}) = \lim_{n\to\infty} \frac{1}{n^2} = \frac{1}{\infty^2} = 0$. So we have shown that its variance converges to 0. Since both its expected value and variance both converge to 0 as $n \to \infty$ then we know that the minimum order statistic converges to 0 as $n \to \infty$ if $X \sim$ Exp$(\lambda = 1)$.

# Question 1A Formula

## Proof

$$P(X_{(1)} \leq Q_p \leq X_{(n)}) \geq \alpha$$
$$P(Q_p \leq X_{(n)}) - P(Q_p \leq X_{(1)}) \geq \alpha$$
$$P(F^{-1}(p) \leq X_{(n)}) - P(F^{-1}(p) \leq X_{(1)}) \geq \alpha$$
$$P(p \leq F(X_{(n)})) - P(p \leq F(X_{(1)})) \geq \alpha$$
$$1 - P(F(X_{(n)}) < p) - (1 - P(F(X_{(1)}) < p)) \geq \alpha$$
$$1 - p^n - (1 - (1 - (1-p)^n)) \geq \alpha$$
$$1 - p^n - (1-p)^n \geq \alpha$$

## Note section

When finding $P(F(X_{(n)}) < p) = F_{X_{(n)}}(p)$ we know we need to find the CDF of our function ($F_{X_{(n)}}(p)$). We also know for a maximum order statistic that $F_{X_{(n)}}(p) = P(X_1 < p) * ... P(X_n < p) = (F(p))^n$ and we know that $F(p) = p$. Therefore, $(F(p))^n = p^n$ so $P(F(X_{(n)}) < p) = p^n$.

When finding $P(F(X_{(1)}) < p) = F_{X_{(1)}}(p)$ we know we need to find the CDF of our function ($F_{X_{(1)}}(p)$). We also know for a minimum order statistic that $F_{X_{(1)}}(p) = 1 - (1 - F(p))^n$ and we know that $F(p) = p$. Therefore, $1 - (1 - F(p)^n) = 1 - (1 - p)^n)$ so $P(F(X_{(1)}) < p) = 1 - (1 - p)^n$.